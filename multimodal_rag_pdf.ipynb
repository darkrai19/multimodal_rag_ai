{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e78862-bdc4-4c19-af0e-cf073a89109a",
   "metadata": {},
   "source": [
    "Function to extract text and images from pdf file.\n",
    "The extracted texts will also be already chunked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7a5ee4f-5bb8-42cc-b62e-f6aa01545aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def extract_content(pdf_path, output_dir, resize_width=None, resize_height=None):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    extracted_data = {\n",
    "        \"text\": [],\n",
    "        \"images\": []\n",
    "    }\n",
    "\n",
    "    # Convert PDF pages to images\n",
    "    pages = convert_from_path(pdf_path, dpi=300)\n",
    "\n",
    "    # Initialize RecursiveCharacterTextSplitter for text chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    # Process each page\n",
    "    for i, page in enumerate(pages):\n",
    "        # Save the page as an image\n",
    "        image_path = os.path.join(output_dir, f\"page_{i + 1}.jpg\")\n",
    "\n",
    "        # Resize the image if dimensions are provided\n",
    "        if resize_width or resize_height:\n",
    "            # Keep aspect ratio if only one dimension is provided\n",
    "            if not resize_width:\n",
    "                aspect_ratio = page.width / page.height\n",
    "                resize_width = int(resize_height * aspect_ratio)\n",
    "            elif not resize_height:\n",
    "                aspect_ratio = page.height / page.width\n",
    "                resize_height = int(resize_width / aspect_ratio)\n",
    "\n",
    "            # Resize the image\n",
    "            page = page.resize((resize_width, resize_height), Image.ANTIALIAS)\n",
    "\n",
    "        # Save the image file\n",
    "        page.save(image_path, \"JPEG\")\n",
    "        extracted_data[\"images\"].append(image_path)\n",
    "\n",
    "        # Extract text using pytesseract OCR\n",
    "        raw_text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        chunked_text = text_splitter.split_text(raw_text)\n",
    "        extracted_data[\"text\"].extend(chunked_text)\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da6a35-979f-418e-ade1-cb03aa2f4258",
   "metadata": {},
   "source": [
    "Function to generate embeddings for images and texts\n",
    "This is separated to check if there is a huge difference in speed when generating the text and image embeddings.\n",
    "Additionally, the generate_text_embeddings accepts list of texts, while the generate_image_embeddings accepts file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dceac31d-2532-4c97-b17e-812953fe4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "from colpali_engine.compression.token_pooling import HierarchicalTokenPooler\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "import numpy as np\n",
    "\n",
    "def generate_text_embeddings(text_list, model_name=\"vidore/colqwen2-v1.0\", pool_factor=3):\n",
    "    \"\"\"\n",
    "    Generate pooled embeddings for text chunks using the ColQwen2 model on GPU.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ColQwen2.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": device},\n",
    "        attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "    ).eval().to(device)\n",
    "    processor = ColQwen2Processor.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    batch = processor.process_queries(text_list).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**batch).cpu().float().numpy()\n",
    "\n",
    "    pooled_embeddings = np.mean(embeddings, axis=1)\n",
    "    pooled_embeddings = pooled_embeddings.tolist()\n",
    "    \n",
    "    return pooled_embeddings\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def generate_image_embeddings(image_paths, model_name=\"vidore/colqwen2-v1.0\"):\n",
    "    \"\"\"\n",
    "    Generate pooled embeddings for image paths using the ColQwen2 model on GPU.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ColQwen2.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": device},\n",
    "        attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "    ).eval().to(device)\n",
    "    processor = ColQwen2Processor.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    images = [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]\n",
    "    processed_images = processor.process_images(images).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**processed_images).cpu().float().numpy()\n",
    "        \n",
    "    pooled_embeddings = np.mean(embeddings, axis=1)\n",
    "    pooled_embeddings = pooled_embeddings.tolist()  \n",
    "\n",
    "    return pooled_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f84a1bf2-ca69-4b56-a91f-6efb434dab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def create_table(db_config):\n",
    "    \"\"\"\n",
    "    Create a single table to store metadata and embeddings.\n",
    "\n",
    "    Args:\n",
    "        db_config (dict): Database configuration with keys 'host', 'database', 'user', 'password'.\n",
    "    \"\"\"\n",
    "    connection = None  # Initialize the connection\n",
    "    try:\n",
    "        # Connect to the PostgreSQL database\n",
    "        connection = psycopg2.connect(**db_config)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Enable pgvector extension\n",
    "        cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "\n",
    "        # Create a combined table for metadata and embeddings\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE embeddings (\n",
    "                id SERIAL PRIMARY KEY,       -- Unique ID for each entry\n",
    "                content TEXT,                -- Text or image link\n",
    "                is_image BOOLEAN,            -- Indicates if the content is an image (True for images, False for text)\n",
    "                embedding VECTOR(128)        -- The pooled embedding (1D vector of 128 dimensions)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # Commit the changes\n",
    "        connection.commit()\n",
    "        print(\"Table successfully created!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b849540-57ea-42a0-ab50-82ea6c6ea142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_embeddings(db_config, text_chunks):\n",
    "    \"\"\"\n",
    "    Insert text embeddings into the database with is_image set to False.\n",
    "\n",
    "    Args:\n",
    "        db_config (dict): Database configuration with keys 'host', 'database', 'user', 'password'.\n",
    "        text_chunks (list): List of text chunks to embed and insert into the database.\n",
    "    \"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        import psycopg2\n",
    "\n",
    "        # Generate text embeddings\n",
    "        embeddings = generate_text_embeddings(text_chunks)\n",
    "\n",
    "        # Connect to the database\n",
    "        connection = psycopg2.connect(**db_config)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Insert text embeddings into the database\n",
    "        for text, embedding in zip(text_chunks, embeddings):\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO embeddings (content, is_image, embedding) VALUES (%s, %s, %s);\",\n",
    "                (text, False, embedding)\n",
    "            )\n",
    "\n",
    "        # Commit the transaction\n",
    "        connection.commit()\n",
    "        print(\"Inserted text embeddings successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting text embeddings: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c943460-b09b-4b9a-b0aa-d1b5200f5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_embeddings(db_config, image_paths):\n",
    "    \"\"\"\n",
    "    Insert image embeddings into the database with is_image set to True.\n",
    "\n",
    "    Args:\n",
    "        db_config (dict): Database configuration with keys 'host', 'database', 'user', 'password'.\n",
    "        image_paths (list): List of image file paths to embed and insert into the database.\n",
    "    \"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        import psycopg2\n",
    "\n",
    "        # Generate image embeddings\n",
    "        embeddings = generate_image_embeddings(image_paths)\n",
    "\n",
    "        # Connect to the database\n",
    "        connection = psycopg2.connect(**db_config)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Insert image embeddings into the database\n",
    "        for image_path, embedding in zip(image_paths, embeddings):\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO embeddings (content, is_image, embedding) VALUES (%s, %s, %s);\",\n",
    "                (image_path, True, embedding)\n",
    "            )\n",
    "\n",
    "        # Commit the transaction\n",
    "        connection.commit()\n",
    "        print(\"Inserted image embeddings successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting image embeddings: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c90365c-a2e9-4221-b14e-6ef4eabef9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_content(db_config, query_embedding, top_k=5, include_images=\"False\"):\n",
    "    \"\"\"\n",
    "    Retrieve the top-K most similar content based on query embedding,\n",
    "    with three modes for filtering: text only, images only, or both.\n",
    "\n",
    "    Args:\n",
    "        db_config (dict): Database configuration with keys 'host', 'database', 'user', 'password'.\n",
    "        query_embedding (list): Query embedding (1D vector of 128 dimensions).\n",
    "        top_k (int): The number of most similar content items to retrieve for each mode.\n",
    "        include_images (str): Mode to filter results - \"False\", \"True\", or \"Both\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing content, distance, and is_image flag.\n",
    "    \"\"\"\n",
    "    connection = None\n",
    "    cursor = None  # Initialize cursor to ensure it exists in case of exceptions\n",
    "    try:\n",
    "        import psycopg2\n",
    "\n",
    "        # Convert numpy embedding to list if needed\n",
    "        if isinstance(query_embedding, np.ndarray):\n",
    "            query_embedding = query_embedding.tolist()\n",
    "\n",
    "        # Connect to the database\n",
    "        connection = psycopg2.connect(**db_config)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Initialize result list\n",
    "        similar_content = []\n",
    "\n",
    "        # Mode: Text Only (include_images=False)\n",
    "        if include_images == \"False\":\n",
    "            retrieve_query = \"\"\"\n",
    "            SELECT content, is_image, embedding <-> %s::vector AS distance\n",
    "            FROM embeddings\n",
    "            WHERE is_image = False\n",
    "            ORDER BY distance ASC\n",
    "            LIMIT %s;\n",
    "            \"\"\"\n",
    "            cursor.execute(retrieve_query, (query_embedding, top_k))\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            for content, is_image, distance in results:\n",
    "                similar_content.append({\n",
    "                    \"content\": content,\n",
    "                    \"is_image\": is_image,\n",
    "                    \"distance\": round(distance, 4)\n",
    "                })\n",
    "\n",
    "        # Mode: Images Only (include_images=True)\n",
    "        elif include_images == \"True\":\n",
    "            retrieve_query = \"\"\"\n",
    "            SELECT content, is_image, embedding <-> %s::vector AS distance\n",
    "            FROM embeddings\n",
    "            WHERE is_image = True\n",
    "            ORDER BY distance ASC\n",
    "            LIMIT %s;\n",
    "            \"\"\"\n",
    "            cursor.execute(retrieve_query, (query_embedding, top_k))\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            for content, is_image, distance in results:\n",
    "                similar_content.append({\n",
    "                    \"content\": content,\n",
    "                    \"is_image\": is_image,\n",
    "                    \"distance\": round(distance, 4)\n",
    "                })\n",
    "\n",
    "        # Mode: Both (include_images=\"Both\")\n",
    "        elif include_images == \"Both\":\n",
    "            # Retrieve top-k text results\n",
    "            retrieve_text_query = \"\"\"\n",
    "            SELECT content, is_image, embedding <-> %s::vector AS distance\n",
    "            FROM embeddings\n",
    "            WHERE is_image = False\n",
    "            ORDER BY distance ASC\n",
    "            LIMIT %s;\n",
    "            \"\"\"\n",
    "            cursor.execute(retrieve_text_query, (query_embedding, top_k))\n",
    "            text_results = cursor.fetchall()\n",
    "\n",
    "            for content, is_image, distance in text_results:\n",
    "                similar_content.append({\n",
    "                    \"content\": content,\n",
    "                    \"is_image\": is_image,\n",
    "                    \"distance\": round(distance, 4)\n",
    "                })\n",
    "\n",
    "            # Retrieve top-k image results\n",
    "            retrieve_image_query = \"\"\"\n",
    "            SELECT content, is_image, embedding <-> %s::vector AS distance\n",
    "            FROM embeddings\n",
    "            WHERE is_image = True\n",
    "            ORDER BY distance ASC\n",
    "            LIMIT %s;\n",
    "            \"\"\"\n",
    "            cursor.execute(retrieve_image_query, (query_embedding, top_k))\n",
    "            image_results = cursor.fetchall()\n",
    "\n",
    "            for content, is_image, distance in image_results:\n",
    "                similar_content.append({\n",
    "                    \"content\": content,\n",
    "                    \"is_image\": is_image,\n",
    "                    \"distance\": round(distance, 4)\n",
    "                })\n",
    "\n",
    "        return similar_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving similar content: {e}\")\n",
    "        return []\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cc2dac7-1b7c-4909-a808-dcfa815d7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(db_config):\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        create_index_query = \"\"\"\n",
    "        CREATE INDEX hnsw_index\n",
    "        ON embeddings\n",
    "        USING hnsw (embedding vector_l2_ops)\n",
    "        WITH (m = 16, ef_construction = 200);\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.execute(create_index_query)\n",
    "        conn.commit()\n",
    "\n",
    "        print(\"HNSW index created successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ebe4d0d-c60c-45eb-a815-29ae18f1393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(query, db_config, top_k=5):\n",
    "    \"\"\"\n",
    "    Create a prompt for the LLM based on the query by generating embeddings and performing similarity searches.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        db_config (dict): Database configuration with keys 'host', 'database', 'user', 'password'.\n",
    "        top_k (int): The number of most similar content items to retrieve for each type.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt for the LLM.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate query embeddings\n",
    "    query_embedding = generate_text_embeddings([query])[0]  # Generate embedding for the query text\n",
    "\n",
    "    # Step 2: Retrieve similar content\n",
    "    retrieved_content = retrieve_similar_content(db_config, query_embedding, top_k=top_k, include_images=\"Both\")\n",
    "\n",
    "    # Step 3: Format the retrieved content into a prompt\n",
    "    text_matches = [item for item in retrieved_content if not item[\"is_image\"]]\n",
    "    image_matches = [item for item in retrieved_content if item[\"is_image\"]]\n",
    "\n",
    "    prompt = f\"User query: {query}\\n\\n\"\n",
    "\n",
    "    prompt += \"Here are the most relevant text results:\\n\"\n",
    "    for text in text_matches:\n",
    "        prompt += f\"- {str(text['content'])} (Similarity Score: {text['distance']})\\n\"\n",
    "\n",
    "    prompt += \"\\nHere are the most relevant images:\\n\"\n",
    "    for image in image_matches:\n",
    "        prompt += f\"- {str(image['content'])} (Similarity Score: {image['distance']})\\n\"\n",
    "\n",
    "    prompt += \"\\nGenerate a response based on the query and the above context.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c417fa1-de73-4aad-8018-3b133675dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_image(query, db_config, top_k = 1):\n",
    "    query_embedding = generate_text_embeddings([query])[0]\n",
    "    retrieved_content = retrieve_similar_content(db_config, query_embedding, top_k=top_k, include_images=\"True\")\n",
    "\n",
    "    return retrieved_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a0c28d-57aa-40c4-a0cb-1158569fdd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "def generate_llm_response(query, db_config, model=\"gemma3:4b\", top_k=5):\n",
    "    \"\"\"\n",
    "    Generate a response using the LLM based on the query by calling create_prompt.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        db_config (dict): Database configuration with keys 'host', 'database', 'user', 'password'.\n",
    "        model (str): The model name to use via Ollama (default: gemma3:4b).\n",
    "        top_k (int): The number of most similar content items to retrieve for each type.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the LLM.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Create the prompt\n",
    "        prompt = create_prompt(query, db_config, top_k=top_k)\n",
    "\n",
    "        # Step 2: Send the prompt to Ollama and retrieve the response\n",
    "        response: ChatResponse = chat(model=model, messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            }\n",
    "        ])\n",
    "\n",
    "        # Step 3: Return the generated response\n",
    "        return response.message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45bffa-98f9-4b86-b32a-27ccee112465",
   "metadata": {},
   "source": [
    "Don't edit the code below this part is for testing the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c56ca2c4-8c75-4251-bb53-c90ae49537ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_content(\"AMD Q4'24 Earnings Slides.pdf\", \"C:/Users/raimo/python/multimodal_rag/output_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63072c5b-2f47-43a8-8157-6f9305fd992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table successfully created!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "db_config = dotenv_values('db_config.env')\n",
    "create_table(db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "453e848a-4a8a-448c-8f4c-4dc9d143b6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b578f17a2f544a2b82c73ab8dd6346d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f409ea937d43dfa244c2f0f856d0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted text embeddings successfully!\n"
     ]
    }
   ],
   "source": [
    "load_text_embeddings(db_config, data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3bb9347e-1d00-4071-81ad-e967ab3022b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e98f8f410f450687d30e023deb35a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bcc41175684b4f860d925d57782869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted image embeddings successfully!\n"
     ]
    }
   ],
   "source": [
    "load_image_embeddings(db_config, data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e41c727-7610-4e47-b325-3bda44ea858a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e0823d406c4816bcbb4c2a1d025381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565a93b2928d4b58b92918978d25b743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_query = [\"What is the earnings per share of the company for year 2024?\"]\n",
    "sample_query_embeddings = generate_text_embeddings(sample_query)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c40490e-9f77-4833-8a4d-6b3addcc27c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'EARNINGS PER SHARE 4 2024\\n\\nGAAP\\n\\nNon-GAAP\"\\n\\n$1.09\\n$0.77\\n$0.41\\nQ4 2023 Q4 2024 Q4 2023 Q4 2024\\n\\n= GAAP net income of $482 million = Record non-GAAP net income of $1.8 billion\\n= GAAP EPS down 29% y/y, primarily driven by higher = Non-GAAP EPS up 42% y/y, primarily driven by higher\\n\\nrevenue and gross margin, more than offset by higher revenue and gross margin, partially offset by higher',\n",
       "  'is_image': False,\n",
       "  'distance': 0.4738},\n",
       " {'content': 'EARNINGS PER SHARE FY 2024\\n\\nGAAP\\n\\n$1.00\\n$0.53\\nFY 2023 FY 2024\\n= GAAP net income of $1.6 billion, up 92% y/y \"\\n= GAAP EPS of $1.00, up 89% y/y, primarily driven 8\\n\\nby higher revenue and gross margin, and lower\\namortization of acquisition-related intangible\\nassets, partially offset by higher operating\\nexpenses and a one-time tax provision\\n\\nNon-GAAP’\\n$3.31\\n\\n$2.65\\n\\nFY 2023 FY 2024',\n",
       "  'is_image': False,\n",
       "  'distance': 0.4782},\n",
       " {'content': 'Non GAAP net income / earnings per share $1,777 $ 1.09 $1,249 $ 0.77 $1,504 $ 0.92 $5,420 $ 3.31 $4,302 $ 2.65\\nShares used in earnings per share calculation\\nShares used in per share calculation (GAAP and Non GAAP) 1,634 1,628 1,636 1,637 1,625',\n",
       "  'is_image': False,\n",
       "  'distance': 0.5136},\n",
       " {'content': 'OPERATING INCOME 4 2024\\n\\nGAAP Non-GAAP\"\\n\\n$2.0B\\n\\n$1.4B\\n$0.9B\\n\\n$0.3B\\ni ee\\n\\nQ4 2023 Q4 2024 Q4 2023 Q4 2024\\n\\nIncrease in both GAAP and non-GAAP operating income driven by higher revenue and gross margin\\n\\n1. See Appendices for GAAP to Non-GAAP reconc iliation',\n",
       "  'is_image': False,\n",
       "  'distance': 0.5264},\n",
       " {'content': 'GROSS MARGIN FY 2024\\n\\nGAAP Non-GAAP\"\\n\\n53%\\n\\n49% 30%\\n\\n46%\\n\\nFY 2023 FY 2024 FY 2023 FY 2024\\n\\nIncrease in gross margin driven by a favorable shift in revenue mix with higher Data Center and\\nClient revenues, lower Gaming revenue, partially offset by the impact of lower Embedded revenue\\n\\n1. See Appendices for GAAP to Non-GAAP reconc iliation',\n",
       "  'is_image': False,\n",
       "  'distance': 0.5267}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_similar_content(db_config, sample_query_embeddings, top_k=5, include_images=\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d0ad1f5-dc2f-4e4c-bb47-76e09f2b8c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW index created successfully!\n"
     ]
    }
   ],
   "source": [
    "create_index(db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97b3f719-02a9-4c28-a635-2564619ae86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46e7fa2a0f446eaab5a8e6c6e30ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb2b09b0cfc48b2b9d88b6013c80795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided text results, AMD has significant growth opportunities across diverse markets. The presentation specifically highlights AMD’s expected first quarter 2025 financial outlook, including revenue, non-GAAP gross margin, and operating expenses. However, it’s important to note that these forward-looking statements are based on current expectations as of February 4, 2025, and are subject to potential changes due to factors like competition with Intel. The presentation also details forward-looking non-GAAP measures concerning AMD’s financial outlook.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradio_interface('What are the growth opportunities?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d71252-0d6a-4849-844f-52d65f19185a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "db_config = dotenv_values('db_config.env')\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Gradio wrapper for query handling\n",
    "def gradio_interface(query):\n",
    "    try:\n",
    "        # Generate response using Gemma3:4b model\n",
    "        response = generate_llm_response(query, db_config, model=\"gemma3:4b\", top_k=5)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Create the enhanced Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    # Title Section\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ## Multimodal RAG\n",
    "        Experience the power of multimodal retrieval and response generation using cutting-edge AI technology. \n",
    "        Enter your query below and receive contextually relevant insights based on text and image data.\n",
    "        \"\"\",\n",
    "        elem_classes=\"title-section\"\n",
    "    )\n",
    "    \n",
    "    # Query Input Section\n",
    "    with gr.Row(elem_classes=\"input-section\"):\n",
    "        query_input = gr.Textbox(\n",
    "            label=\"Your Query\", \n",
    "            placeholder=\"Type your question here...\",\n",
    "            lines=2,\n",
    "            elem_classes=\"query-input\"\n",
    "        )\n",
    "\n",
    "    # Response Output Section\n",
    "    response_output = gr.Textbox(\n",
    "        label=\"Response\", \n",
    "        placeholder=\"Generated response will appear here.\",\n",
    "        lines=8,\n",
    "        interactive=False,\n",
    "        elem_classes=\"response-output\"\n",
    "    )\n",
    "\n",
    "    # Submit Button Section\n",
    "    submit_button = gr.Button(\n",
    "        \"Generate Response\", \n",
    "        elem_classes=\"submit-button\"\n",
    "    )\n",
    "\n",
    "    # Link the input and output with the function\n",
    "    submit_button.click(fn=gradio_interface, inputs=query_input, outputs=response_output)\n",
    "\n",
    "# Custom CSS for styling\n",
    "demo.css = \"\"\"\n",
    ".title-section {\n",
    "    font-size: 1.5em;\n",
    "    color: #1e293b;\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    "\n",
    ".input-section {\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    "\n",
    ".query-input {\n",
    "    width: 100%;\n",
    "    padding: 10px;\n",
    "    border-radius: 5px;\n",
    "    border: 1px solid #1e293b;\n",
    "}\n",
    "\n",
    ".response-output {\n",
    "    background-color: #f1f5f9;\n",
    "    padding: 10px;\n",
    "    border-radius: 5px;\n",
    "    border: 1px solid #1e293b;\n",
    "}\n",
    "\n",
    ".submit-button {\n",
    "    background-color: #2563eb;\n",
    "    color: #ffffff;\n",
    "    border-radius: 5px;\n",
    "    padding: 10px 20px;\n",
    "    font-size: 1em;\n",
    "    cursor: pointer;\n",
    "    text-transform: uppercase;\n",
    "    box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\n",
    "}\n",
    "\n",
    ".submit-button:hover {\n",
    "    background-color: #1d4ed8;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Launch the enhanced Gradio UI\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e1f77bd-5e14-4764-95d3-29eec4a64dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Wrapper function for handling query\n",
    "def wrapper_function(query):\n",
    "    db_config = db_config\n",
    "    \n",
    "    try:\n",
    "        # Text Response\n",
    "        response_text = generate_llm_response(query, db_config, model=\"gemma3:4b\", top_k=5)\n",
    "        \n",
    "        # Image Retrieval\n",
    "        query_embedding = generate_text_embeddings([query])[0]\n",
    "        retrieved_images = retrieve_similar_content(db_config, query_embedding, top_k=1, include_images=\"True\")\n",
    "        closest_image_path = retrieved_images[0][\"content\"] if retrieved_images else None\n",
    "\n",
    "        return response_text, closest_image_path\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8b60a09-5302-46c5-b2fc-37412ded777a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gallery' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mTab(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload PDF\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     36\u001b[0m     pdf_input \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mFile(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload PDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 37\u001b[0m     extracted_images \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mGallery(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstyle(grid\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m], height\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Query Input Section\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mTab(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Gallery' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Load database configuration\n",
    "db_config = dotenv_values('db_config.env')\n",
    "\n",
    "# Wrapper function for handling query\n",
    "def wrapper_function(query):\n",
    "    try:\n",
    "        # Text Response\n",
    "        response_text = generate_llm_response(query, db_config, model=\"gemma3:4b\", top_k=5)\n",
    "        \n",
    "        # Image Retrieval\n",
    "        query_embedding = generate_text_embeddings([query])[0]\n",
    "        retrieved_images = retrieve_similar_content(db_config, query_embedding, top_k=1, include_images=\"True\")\n",
    "        closest_image_path = retrieved_images[0][\"content\"] if retrieved_images else None\n",
    "\n",
    "        return response_text, closest_image_path\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", None\n",
    "\n",
    "# Create the enhanced Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    # Title Section\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ## Multimodal RAG\n",
    "        Experience the power of multimodal retrieval and response generation using cutting-edge AI technology. \n",
    "        Upload a PDF, enter your query, and receive contextually relevant insights based on text and image data.\n",
    "        \"\"\",\n",
    "        elem_classes=\"title-section\"\n",
    "    )\n",
    "\n",
    "    # File Upload Section\n",
    "    with gr.Tab(\"Upload PDF\"):\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        extracted_images = gr.Gallery(label=\"Extracted Images\").style(grid=[3], height=\"auto\")\n",
    "\n",
    "    # Query Input Section\n",
    "    with gr.Tab(\"Query\"):\n",
    "        query_input = gr.Textbox(\n",
    "            label=\"Your Query\", \n",
    "            placeholder=\"Type your question here...\",\n",
    "            lines=2,\n",
    "            elem_classes=\"query-input\"\n",
    "        )\n",
    "        response_output = gr.Textbox(\n",
    "            label=\"Generated Text Response\", \n",
    "            placeholder=\"Generated response will appear here.\",\n",
    "            lines=8,\n",
    "            interactive=False,\n",
    "            elem_classes=\"response-output\"\n",
    "        )\n",
    "        image_output = gr.Image(label=\"Closest Matching Image\", type=\"filepath\")\n",
    "        query_history = gr.Textbox(\n",
    "            label=\"Query History\", \n",
    "            interactive=False, \n",
    "            lines=10, \n",
    "            placeholder=\"Your past queries will appear here.\"\n",
    "        )\n",
    "\n",
    "    # Submit Button Section\n",
    "    submit_button = gr.Button(\n",
    "        \"Generate Results\", \n",
    "        elem_classes=\"submit-button\"\n",
    "    )\n",
    "\n",
    "    # Feedback Section\n",
    "    feedback = gr.Textbox(label=\"Feedback\", placeholder=\"Provide your feedback here...\")\n",
    "    submit_feedback = gr.Button(\"Submit Feedback\")\n",
    "\n",
    "    # Link the input and outputs to the wrapper function\n",
    "    submit_button.click(fn=wrapper_function, inputs=query_input, outputs=[response_output, image_output])\n",
    "\n",
    "# Custom CSS for styling\n",
    "demo.css = \"\"\"\n",
    ".title-section {\n",
    "    font-size: 1.5em;\n",
    "    color: #1e293b;\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    "\n",
    ".input-section {\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    "\n",
    ".query-input {\n",
    "    width: 100%;\n",
    "    padding: 10px;\n",
    "    border-radius: 5px;\n",
    "    border: 1px solid #1e293b;\n",
    "}\n",
    "\n",
    ".response-output {\n",
    "    background-color: #f1f5f9;\n",
    "    padding: 10px;\n",
    "    border-radius: 5px;\n",
    "    border: 1px solid #1e293b;\n",
    "}\n",
    "\n",
    ".submit-button {\n",
    "    background-color: #2563eb;\n",
    "    color: #ffffff;\n",
    "    border-radius: 5px;\n",
    "    padding: 10px 20px;\n",
    "    font-size: 1em;\n",
    "    cursor: pointer;\n",
    "    text-transform: uppercase;\n",
    "    box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\n",
    "}\n",
    "\n",
    ".submit-button:hover {\n",
    "    background-color: #1d4ed8;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Launch the enhanced Gradio UI\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
